{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7641 ML Assignment 4: Markov Decision Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import hiive.mdptoolbox.example\n",
    "import hiive.mdptoolbox as mdptoolbox\n",
    "from hiive.mdptoolbox.mdp import ValueIteration, PolicyIteration, QLearning\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from IPython.display import clear_output, display\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {\n",
    "    b'S': 'b',\n",
    "    b'F': 'w',\n",
    "    b'H': 'k',\n",
    "    b'G': 'g'\n",
    "}\n",
    "\n",
    "directions = {\n",
    "            0: '←',\n",
    "            1: '↓',\n",
    "            2: '→',\n",
    "            3: '↑'\n",
    "}\n",
    "\n",
    "def plot_lake(env, policy=None, title='Frozen Lake'):\n",
    "    squares = env.nrow\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    ax = fig.add_subplot(111, xlim=(-.01, squares+0.01), ylim=(-.01, squares+0.01))\n",
    "    plt.title(title, fontsize=16, weight='bold', y=1.01)\n",
    "    for i in range(squares):\n",
    "        for j in range(squares):\n",
    "            y = squares - i - 1\n",
    "            x = j\n",
    "            p = plt.Rectangle([x, y], 1, 1, linewidth=1, edgecolor='k')\n",
    "            p.set_facecolor(colors[env.desc[i,j]])\n",
    "            ax.add_patch(p)\n",
    "            \n",
    "            if policy is not None:\n",
    "                text = ax.text(x+0.5, y+0.5, directions[policy[i, j]],\n",
    "                               horizontalalignment='center', size=25, verticalalignment='center',\n",
    "                               color='k')\n",
    "            \n",
    "    plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code based on:\n",
    "# https://medium.com/analytics-vidhya/solving-the-frozenlake-environment-from-openai-gym-using-value-iteration-5a078dffe438\n",
    "def get_score(env, policy, printInfo=False, episodes=1000):\n",
    "    misses = 0\n",
    "    steps_list = []\n",
    "    for episode in range(episodes):\n",
    "        observation = env.reset()\n",
    "        steps=0\n",
    "        while True:\n",
    "            action = policy[observation]\n",
    "            observation, reward, done, _ = env.step(action)\n",
    "            steps+=1\n",
    "            if done and reward == 1:\n",
    "                # print('You have got the Frisbee after {} steps'.format(steps))\n",
    "                steps_list.append(steps)\n",
    "                break\n",
    "            elif done and reward == 0:\n",
    "                # print(\"You fell in a hole!\")\n",
    "                misses += 1\n",
    "                break\n",
    "    ave_steps = np.mean(steps_list)\n",
    "    std_steps = np.std(steps_list)\n",
    "    pct_fail  = (misses/episodes)* 100\n",
    "    \n",
    "    if (printInfo):\n",
    "        print('----------------------------------------------')\n",
    "        print('You took an average of {:.0f} steps to get the frisbee'.format(ave_steps))\n",
    "        print('And you fell in the hole {:.2f} % of the times'.format(pct_fail))\n",
    "        print('----------------------------------------------')\n",
    "  \n",
    "    return ave_steps, std_steps, pct_fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code based on:\n",
    "# https://medium.com/analytics-vidhya/solving-the-frozenlake-environment-from-openai-gym-using-value-iteration-5a078dffe438\n",
    "def get_policy(env,stateValue, lmbda=0.9):\n",
    "    policy = [0 for i in range(env.nS)]\n",
    "    for state in range(env.nS):\n",
    "        action_values = []\n",
    "        for action in range(env.nA):\n",
    "            action_value = 0\n",
    "            for i in range(len(env.P[state][action])):\n",
    "                prob, next_state, r, _ = env.P[state][action][i]\n",
    "                action_value += prob * (r + lmbda * stateValue[next_state])\n",
    "            action_values.append(action_value)\n",
    "        best_action = np.argmax(np.asarray(action_values))\n",
    "        policy[state] = best_action\n",
    "    return policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code based on:\n",
    "# https://medium.com/analytics-vidhya/solving-the-frozenlake-environment-from-openai-gym-using-value-iteration-5a078dffe438\n",
    "def get_score(env, policy, printInfo=False, episodes=1000):\n",
    "    misses = 0\n",
    "    steps_list = []\n",
    "    for episode in range(episodes):\n",
    "        observation = env.reset()\n",
    "        steps=0\n",
    "        while True:\n",
    "            action = policy[observation]\n",
    "            observation, reward, done, _ = env.step(action)\n",
    "            steps+=1\n",
    "            if done and reward == 1:\n",
    "                # print('You have got the Frisbee after {} steps'.format(steps))\n",
    "                steps_list.append(steps)\n",
    "                break\n",
    "            elif done and reward == 0:\n",
    "                # print(\"You fell in a hole!\")\n",
    "                misses += 1\n",
    "                break\n",
    "    ave_steps = np.mean(steps_list)\n",
    "    std_steps = np.std(steps_list)\n",
    "    pct_fail  = (misses/episodes)* 100\n",
    "    \n",
    "    if (printInfo):\n",
    "        print('----------------------------------------------')\n",
    "        print('You took an average of {:.0f} steps to get the frisbee'.format(ave_steps))\n",
    "        print('And you fell in the hole {:.2f} % of the times'.format(pct_fail))\n",
    "        print('----------------------------------------------')\n",
    "  \n",
    "    return ave_steps, std_steps, pct_fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {\n",
    "    0: 'g',\n",
    "    1: 'k'\n",
    "}\n",
    "\n",
    "labels = {\n",
    "    0: 'W',\n",
    "    1: 'C',\n",
    "}\n",
    "\n",
    "def plot_forest(policy, title='Forest Management'):\n",
    "    rows = 25\n",
    "    cols = 25\n",
    "    \n",
    "    # reshape policy array to be 2-D - assumes 500 states...\n",
    "    policy = np.array(list(policy)).reshape(rows,cols)\n",
    "    \n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(111, xlim=(-.01, cols+0.01), ylim = (-.01, rows+0.01))\n",
    "    plt.title(title, fontsize=16, weight='bold', y=1.01)\n",
    "    \n",
    "    for i in range(25):\n",
    "        for j in range(25):\n",
    "            y = 25 - i - 1\n",
    "            x = j\n",
    "            p = plt.Rectangle([x, y], 1, 1, linewidth=1, edgecolor='k')\n",
    "            p.set_facecolor(colors[policy[i,j]])\n",
    "            ax.add_patch(p)\n",
    "            \n",
    "            text = ax.text(x+0.5, y+0.5, labels[policy[i, j]],\n",
    "                           horizontalalignment='center', size=10, verticalalignment='center', color='w')\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.savefig('./forest/' + title + '.png', dpi=400)\n",
    "    \n",
    "    \n",
    "#plot_forest(bestPolicy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_QL(dependent, independent, title=None, logscale=False):\n",
    "    if dependent not in interest:\n",
    "        print('Dependent variable not available')\n",
    "        return\n",
    "    if independent not in interest:\n",
    "        print('Independent variable not available')\n",
    "        return\n",
    "    \n",
    "    x = np.unique(df[dependent])\n",
    "    y = []\n",
    "    \n",
    "    for i in x:\n",
    "        y.append(df.loc[df[dependent] == i][independent].mean())\n",
    "        \n",
    "    fig = plt.figure(figsize=(6,4))\n",
    "    plt.plot(x, y, 'o-')\n",
    "    \n",
    "    if title == None:\n",
    "        title = independent + ' vs. ' + dependent\n",
    "    plt.title(title, fontsize=15)\n",
    "    plt.xlabel(dependent)\n",
    "    plt.ylabel(independent)\n",
    "    plt.grid(True)\n",
    "    if logscale:\n",
    "        plt.xscale('log')\n",
    "    \n",
    "    title='QL_' + independent + '_vs_' + dependent\n",
    "    plt.savefig('./frozen/' + title + '.png', dpi=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP Problem 1: Forest Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T,R = hiive.mdptoolbox.example.forest(S=625)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valueIteration(t, r, gammas, epsilons, showResults=False, max_iterations=100000):\n",
    "    t0 = time.time()\n",
    "    \n",
    "    # create data structure to save off\n",
    "    columns = ['gamma', 'epsilon', 'time', 'iterations', 'reward', 'average_steps', 'steps_stddev', 'success_pct', 'policy', 'mean_rewards', 'max_rewards', 'error']\n",
    "    data = pd.DataFrame(0.0, index=np.arange(len(gammas)*len(epsilons)), columns=columns)\n",
    "    \n",
    "    print('Gamma,\\tEps,\\tTime,\\tIter,\\tReward')\n",
    "    print(80*'_')\n",
    "    \n",
    "    testNum = 0\n",
    "    for g in gammas:\n",
    "        for e in epsilons:\n",
    "            test = ValueIteration(t, r, gamma=g, epsilon=e, max_iter=max_iterations)\n",
    "            \n",
    "            runs  = test.run()\n",
    "            Time  = runs[-1]['Time']\n",
    "            iters = runs[-1]['Iteration']\n",
    "            maxR  = runs[-1]['Max V']\n",
    "            \n",
    "            max_rewards, mean_rewards, errors = [], [], []\n",
    "            for run in runs:\n",
    "                max_rewards.append(run['Max V'])\n",
    "                mean_rewards.append(run['Mean V'])\n",
    "                errors.append(run['Error'])\n",
    "            \n",
    "            policy = np.array(test.policy)\n",
    "            \n",
    "            data['gamma'][testNum]        = g\n",
    "            data['epsilon'][testNum]      = e\n",
    "            data['time'][testNum]         = Time\n",
    "            data['iterations'][testNum]   = iters\n",
    "            data['reward'][testNum]       = maxR\n",
    "            data['mean_rewards'][testNum] = {tuple(mean_rewards)}\n",
    "            data['max_rewards'][testNum]  = {tuple(max_rewards)}\n",
    "            data['error'][testNum]        = {tuple(errors)}\n",
    "            data['policy'][testNum]       = {test.policy}\n",
    "            \n",
    "            print('%.2f,\\t%.0E,\\t%.2f,\\t%d,\\t%f' % (g, e, Time, iters, maxR))\n",
    "            \n",
    "            testNum = testNum + 1\n",
    "        \n",
    "    endTime = time.time() - t0\n",
    "    print('Time taken: %.2f' % endTime)\n",
    "    \n",
    "    # See differences in policy\n",
    "    policies = data['policy']\n",
    "    \n",
    "    # replace all NaN's\n",
    "    data.fillna(0, inplace=True)\n",
    "    data.head()\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas   = [0.1, 0.3, 0.6, 0.9, 0.9999999]\n",
    "epsilons = [1e-2, 1e-3, 1e-8, 1e-12]\n",
    "vi_data  = valueIteration(T, R, gammas, epsilons, showResults=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi_data.plot(x='gamma', y='reward', title=\"Reward vs. Gamma\")\n",
    "plt.grid()\n",
    "\n",
    "vi_data.plot(x='iterations', y='reward', title=\"Reward vs. Iterations\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the highest score\n",
    "bestRun = vi_data['reward'].argmax()\n",
    "bestPolicy = vi_data['policy'][bestRun]\n",
    "\n",
    "title='Forest Management VI Optimal Policy'\n",
    "plot_forest(bestPolicy, title)\n",
    "print('Best Result:\\n\\tReward = %.2f\\n\\tGamma = %.7f\\n\\tEpsilon= %.E' % (vi_data['reward'].max(), vi_data['gamma'][bestRun], vi_data['epsilon'][bestRun]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policyIteration(t, r, gammas, showResults=False, max_iterations=100000):\n",
    "    t0 = time.time()\n",
    "    \n",
    "    # create data structure to save off\n",
    "    columns = ['gamma', 'epsilon', 'time', 'iterations', 'reward', 'average_steps', 'steps_stddev', 'success_pct', 'policy', 'mean_rewards', 'max_rewards', 'error']\n",
    "    data = pd.DataFrame(0.0, index=np.arange(len(gammas)), columns=columns)\n",
    "    \n",
    "    print('gamma,\\ttime,\\titer,\\treward')\n",
    "    print(80*'_')\n",
    "    \n",
    "    testnum = 0\n",
    "    for g in gammas:\n",
    "        test = PolicyIteration(t, r, gamma=g, max_iter=max_iterations, eval_type=\"matrix\") # eval_type=\"iterative\"\n",
    "        \n",
    "        runs  = test.run()\n",
    "        Time  = test.time\n",
    "        iters = test.iter\n",
    "        maxr  = runs[-1]['Max V']\n",
    "                \n",
    "        max_rewards, mean_rewards, errors = [], [], []\n",
    "        for run in runs:\n",
    "            max_rewards.append(run['Max V'])\n",
    "            mean_rewards.append(run['Mean V'])\n",
    "            errors.append(run['Error'])\n",
    "        \n",
    "        data['gamma'][testnum]        = g\n",
    "        data['time'][testnum]         = Time\n",
    "        data['iterations'][testnum]   = iters\n",
    "        data['reward'][testnum]       = maxr\n",
    "        data['mean_rewards'][testnum] = {tuple(mean_rewards)}\n",
    "        data['max_rewards'][testnum]  = {tuple(max_rewards)}\n",
    "        data['error'][testnum]        = {tuple(errors)}\n",
    "        data['policy'][testnum]       = {test.policy}\n",
    "        \n",
    "        print('%.2f,\\t%.2f,\\t%d,\\t%f' % (g, Time, iters, maxr))\n",
    "        \n",
    "        if showResults:\n",
    "            plot_forest(policy, title)\n",
    "            pass\n",
    "        \n",
    "        testnum = testnum + 1\n",
    "        \n",
    "    endTime = time.time() - t0\n",
    "    print('Time taken: %.2f' % endTime)\n",
    "    \n",
    "    # see differences in policy\n",
    "    policies = data['policy']\n",
    "        \n",
    "    # replace all nan's\n",
    "    data.fillna(0, inplace=True)\n",
    "    data.head()\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas   = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.999]\n",
    "pi_data  = policyIteration(T, R, gammas, showResults=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_data.plot(x='gamma', y='reward', title='Rewards vs. Gamma')\n",
    "plt.grid()\n",
    "\n",
    "pi_data.plot(x='iterations', y='reward', title='Rewards vs. Iterations')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the highest score\n",
    "bestRun = pi_data['reward'].argmax()\n",
    "bestPolicy = pi_data['policy'][bestRun]\n",
    "\n",
    "title='Forest Management PI Optimal Policy'\n",
    "plot_forest(bestPolicy, title)\n",
    "\n",
    "print('Best Result:\\n\\tReward = %.2f\\n\\tGamma = %.3f' % (pi_data['reward'].max(), pi_data['gamma'][bestRun]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qLearning(t, r, gammas, alphas, alpha_decays=[0.99], epsilon_decays=[0.99], n_iterations=[10000000], showResults=False):\n",
    "    # create data structure to save off\n",
    "    columns = ['gamma', 'alpha', 'alpha_decay', 'epsilon_decay', 'iterations', 'time', 'reward', 'average_steps', 'steps_stddev', 'success_pct', 'policy', 'mean_rewards', 'max_rewards', 'error']\n",
    "    numTests = len(gammas)*len(alphas)*len(alpha_decays)*len(epsilon_decays)*len(n_iterations)\n",
    "    data = pd.DataFrame(0.0, index=np.arange(numTests), columns=columns)\n",
    "    \n",
    "    print('Gamma,\\tAlpha,\\tTime,\\tIter,\\tReward')\n",
    "    print(80*'_')\n",
    "    \n",
    "    testNum = 0\n",
    "    for g in gammas:\n",
    "        for a in alphas:\n",
    "            for a_decay in alpha_decays:\n",
    "                for e_decay in epsilon_decays:\n",
    "                    for n in n_iterations:\n",
    "                        print('Test Num %d/%d' %(testNum+1, numTests))\n",
    "                        print('Gamma: %.2f,\\tAlpha: %.2f,\\tAlpha Decay:%.3f,\\tEpsilon Decay:%.3f,\\tIterations:%d' \n",
    "                             %(g, a, a_decay, e_decay, n))\n",
    "                        \n",
    "                        test = QLearning(t, r, gamma=g, alpha=a, alpha_decay=a_decay, epsilon_decay=e_decay, n_iter=n)\n",
    "                        \n",
    "                        runs  = test.run()\n",
    "                        time  = runs[-1]['Time']\n",
    "                        iters = runs[-1]['Iteration']\n",
    "                        maxR  = runs[-1]['Max V']\n",
    "                        \n",
    "                        max_rewards, mean_rewards, errors = [], [], []\n",
    "                        for run in runs:\n",
    "                            max_rewards.append(run['Max V'])\n",
    "                            mean_rewards.append(run['Mean V'])\n",
    "                            errors.append(run['Error'])\n",
    "                        \n",
    "                        #policy = np.array(test.policy)\n",
    "                        #policy = policy.reshape(4,4)\n",
    "                        \n",
    "                        data['gamma'][testNum]         = g\n",
    "                        data['alpha'][testNum]         = a\n",
    "                        data['alpha_decay'][testNum]   = a_decay\n",
    "                        data['epsilon_decay'][testNum] = e_decay\n",
    "                        data['time'][testNum]          = time\n",
    "                        data['iterations'][testNum]    = iters\n",
    "                        data['reward'][testNum]        = maxR\n",
    "                        data['mean_rewards'][testNum]  = {tuple(mean_rewards)}\n",
    "                        data['max_rewards'][testNum]   = {tuple(max_rewards)}\n",
    "                        data['error'][testNum]         = {tuple(errors)}\n",
    "                        data['policy'][testNum]        = {test.policy}\n",
    "                        \n",
    "                        print('%.2f,\\t%.2f,\\t%.2f,\\t%d,\\t%f' % (g, a, time, iters, maxR))\n",
    "                        \n",
    "                        if showResults:\n",
    "                            pass\n",
    "                        \n",
    "                        testNum = testNum + 1\n",
    "            \n",
    "    # See differences in policy\n",
    "    policies = data['policy']\n",
    "    \n",
    "    '''\n",
    "    for i,p in enumerate(policies):\n",
    "        pol = list(p)[0]\n",
    "        steps, steps_stddev, failures = get_score(env, pol, showResults)\n",
    "        data['average_steps'][i] = steps\n",
    "        data['steps_stddev'][i]  = steps_stddev\n",
    "        data['success_pct'][i]   = 100-failures      \n",
    "    '''\n",
    "        \n",
    "    # replace all NaN's\n",
    "    data.fillna(0, inplace=True)\n",
    "    data.head()\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas         = [0.8, 0.9, 0.99]\n",
    "alphas         = [0.01, 0.1, 0.2]\n",
    "alpha_decays   = [0.9, 0.999]\n",
    "epsilon_decays = [0.9, 0.999]\n",
    "iterations     = [1e5, 1e6, 1e7]\n",
    "\n",
    "ql_data  = qLearning(T, R, gammas, alphas, alpha_decays=alpha_decays, epsilon_decays=epsilon_decays, n_iterations=iterations, showResults=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"white\")\n",
    "fig, ax = plt.subplots(figsize=(8,7))\n",
    "ax.set_title('Correlation Matrix of Q-Learning Parameters', fontsize=20)\n",
    "mask = np.triu(np.ones_like(ql_corr, dtype=np.bool))\n",
    "cmap = sns.diverging_palette(255, 0, as_cmap=True)\n",
    "sns.heatmap(ql_corr, mask=mask, cmap=cmap, square=True, linewidths=0.5, cbar_kws={\"shrink\":.75})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot time vs. Iterations\n",
    "plot_QL('iterations', 'time', title='Mean Time Taken vs. Iterations', logscale=True)\n",
    "\n",
    "plot_QL('iterations', 'reward', title='Mean Reward vs. Iterations', logscale=True)\n",
    "\n",
    "# Plot alpha decay vs success pct\n",
    "plot_QL('alpha_decay', 'reward', title='Mean Reward vs. Alpha Decay')\n",
    "\n",
    "# Plot results vs. gamma\n",
    "plot_QL('gamma', 'reward', title='Mean Reward vs. Gamma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot most successful gamma\n",
    "\n",
    "# find the score with the highest success percentage and get its policy\n",
    "bestRun = ql_data['reward'].argmax()\n",
    "\n",
    "best_policy = ql_data['policy'][bestRun]\n",
    "\n",
    "# reshape the policy since we pulled from a csv file\n",
    "best_policy = best_policy[1:-1]\n",
    "best_policy = eval(best_policy)\n",
    "best_policy = np.array(best_policy)\n",
    "best_policy = best_policy.reshape(25, 25)\n",
    "\n",
    "# plot the policy\n",
    "title='Forest Management QL Optimal Policy'\n",
    "plot_forest(best_policy, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDP Problem 2: Frozen Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup 4x4\n",
    "env = gym.make('FrozenLake-v1').unwrapped\n",
    "\n",
    "env.max_episode_steps=250\n",
    "\n",
    "# Create transition and reward matrices from OpenAI P matrix\n",
    "rows = env.nrow\n",
    "cols = env.ncol\n",
    "T = np.zeros((4, rows*cols, rows*cols))\n",
    "R = np.zeros((4, rows*cols, rows*cols))\n",
    "\n",
    "old_state = np.inf\n",
    "\n",
    "for square in env.P:\n",
    "    for action in env.P[square]:\n",
    "        for i in range(len(env.P[square][action])):\n",
    "            new_state = env.P[square][action][i][1]\n",
    "            if new_state == old_state:\n",
    "                T[action][square][env.P[square][action][i][1]] = T[action][square][old_state] + env.P[square][action][i][0]\n",
    "                R[action][square][env.P[square][action][i][1]] = R[action][square][old_state] + env.P[square][action][i][2]\n",
    "            else:\n",
    "                T[action][square][env.P[square][action][i][1]] = env.P[square][action][i][0]\n",
    "                R[action][square][env.P[square][action][i][1]] = env.P[square][action][i][2]\n",
    "            old_state = env.P[square][action][i][1]\n",
    "            \n",
    "#print(T)\n",
    "#print(R)\n",
    "plot_lake(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valueIteration(t, r, gammas, epsilons, showResults=False, max_iterations=100000):\n",
    "    t0 = time.time()\n",
    "    # create data structure to save off\n",
    "    columns = ['gamma', 'epsilon', 'time', 'iterations', 'reward', 'average_steps', 'steps_stddev', 'success_pct', 'policy', 'mean_rewards', 'max_rewards', 'error']\n",
    "    data = pd.DataFrame(0.0, index=np.arange(len(gammas)*len(epsilons)), columns=columns)\n",
    "    \n",
    "    print('Gamma,\\tEps,\\tTime,\\tIter,\\tReward')\n",
    "    print(80*'_')\n",
    "    \n",
    "    testNum = 0\n",
    "    for g in gammas:\n",
    "        for e in epsilons:\n",
    "            test = ValueIteration(t, r, gamma=g, epsilon=e, max_iter=max_iterations)\n",
    "            \n",
    "            runs  = test.run()\n",
    "            Time  = runs[-1]['Time']\n",
    "            iters = runs[-1]['Iteration']\n",
    "            maxR  = runs[-1]['Max V']\n",
    "            \n",
    "            max_rewards, mean_rewards, errors = [], [], []\n",
    "            for run in runs:\n",
    "                max_rewards.append(run['Max V'])\n",
    "                mean_rewards.append(run['Mean V'])\n",
    "                errors.append(run['Error'])\n",
    "            \n",
    "            policy = np.array(test.policy)\n",
    "            policy = policy.reshape(4,4)\n",
    "            \n",
    "            data['gamma'][testNum]        = g\n",
    "            data['epsilon'][testNum]      = e\n",
    "            data['time'][testNum]         = Time\n",
    "            data['iterations'][testNum]   = iters\n",
    "            data['reward'][testNum]       = maxR\n",
    "            data['mean_rewards'][testNum] = {tuple(mean_rewards)}\n",
    "            data['max_rewards'][testNum]  = {tuple(max_rewards)}\n",
    "            data['error'][testNum]        = {tuple(errors)}\n",
    "            data['policy'][testNum]       = {test.policy}\n",
    "            \n",
    "            print('%.2f,\\t%.0E,\\t%.2f,\\t%d,\\t%f' % (g, e, Time, iters, maxR))\n",
    "            \n",
    "            if showResults:\n",
    "                title = 'FrozenLake_VI_' + str(rows) + 'x' + str(cols) + '_g' + str(g) + '_e' + str(e)\n",
    "                plot_lake(env, policy, title)\n",
    "            \n",
    "            testNum = testNum + 1\n",
    "                \n",
    "    endTime = time.time() - t0\n",
    "    print(\"Time taken: %.2f\" %endTime)\n",
    "    \n",
    "    # See differences in policy\n",
    "    policies = data['policy']\n",
    "    \n",
    "    for i,p in enumerate(policies):\n",
    "        pol = list(p)[0]\n",
    "        steps, steps_stddev, failures = get_score(env, pol, showResults)\n",
    "        data['average_steps'][i] = steps\n",
    "        data['steps_stddev'][i]  = steps_stddev\n",
    "        data['success_pct'][i]   = 100-failures      \n",
    "        \n",
    "    # replace all NaN's\n",
    "    data.fillna(0, inplace=True)\n",
    "    data.head()\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas   = [0.1, 0.3, 0.6, 0.9]\n",
    "epsilons = [1e-2, 1e-5, 1e-8, 1e-12]\n",
    "vi_data  = valueIteration(T, R, gammas, epsilons, showResults=False)\n",
    "\n",
    "interest = ['gamma', 'epsilon', 'time', 'iterations', 'reward']\n",
    "df = vi_data[interest]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare number of iterations with lowest epsilon\n",
    "\n",
    "# extract the average number of steps for each gamma value\n",
    "x = gammas\n",
    "y = []\n",
    "for g in gammas:\n",
    "    y.append(vi_data.loc[vi_data['gamma'] == g]['average_steps'].mean())\n",
    "\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "ax  = sns.barplot(x, y)\n",
    "ax.set_title('Average Steps vs. Gamma')\n",
    "ax.set_xlabel('Gamma')\n",
    "ax.set_ylabel('Average Steps')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare success of runs\n",
    "\n",
    "# extract the average success percentage for each gamma value\n",
    "x = gammas\n",
    "y = []\n",
    "for g in gammas:\n",
    "    y.append(vi_data.loc[vi_data['gamma'] == g]['success_pct'].mean())\n",
    "\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "ax  = sns.barplot(x,y)\n",
    "ax.set_title('Success Percentage vs. Gamma')\n",
    "ax.set_xlabel('Gamma')\n",
    "ax.set_ylabel('Success %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot most successful gamma and epsilon\n",
    "\n",
    "# find the score with the highest success percentage and get its policy\n",
    "bestRun = vi_data['success_pct'].argmax()\n",
    "best_policy = vi_data['policy'][bestRun]\n",
    "best_policy = np.array(list(best_policy)[0])\n",
    "best_policy = best_policy.reshape(rows, cols)\n",
    "\n",
    "# plot the policy\n",
    "title='Frozen Lake VI Optimal Policy'\n",
    "plot_lake(env, best_policy, title)\n",
    "print('Best Result:\\n\\tSuccess = %.2f\\n\\tGamma = %.2f\\n\\tEpsilon= %.E' % (vi_data['success_pct'].max(), vi_data['gamma'][bestRun], vi_data['epsilon'][bestRun]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policyIteration(t, r, gammas, showResults=False, max_iterations=100000):\n",
    "    t0 = time.time()\n",
    "    \n",
    "    # create data structure to save off\n",
    "    columns = ['gamma', 'epsilon', 'time', 'iterations', 'reward', 'average_steps', 'steps_stddev', 'success_pct', 'policy', 'mean_rewards', 'max_rewards', 'error']\n",
    "    data = pd.DataFrame(0.0, index=np.arange(len(gammas)), columns=columns)\n",
    "    \n",
    "    print('gamma,\\ttime,\\titer,\\treward')\n",
    "    print(80*'_')\n",
    "    \n",
    "    testnum = 0\n",
    "    for g in gammas:\n",
    "        test = PolicyIteration(t, r, gamma=g, max_iter=max_iterations, eval_type=\"matrix\") # eval_type=\"iterative\"\n",
    "        \n",
    "        runs  = test.run()\n",
    "        Time  = test.time\n",
    "        iters = test.iter\n",
    "        maxr  = runs[-1]['Max V']\n",
    "                \n",
    "        max_rewards, mean_rewards, errors = [], [], []\n",
    "        for run in runs:\n",
    "            max_rewards.append(run['Max V'])\n",
    "            mean_rewards.append(run['Mean V'])\n",
    "            errors.append(run['Error'])\n",
    "        \n",
    "        policy = np.array(test.policy)\n",
    "        policy = policy.reshape(4,4)\n",
    "        \n",
    "        data['gamma'][testnum]        = g\n",
    "        data['time'][testnum]         = Time\n",
    "        data['iterations'][testnum]   = iters\n",
    "        data['reward'][testnum]       = maxr\n",
    "        data['mean_rewards'][testnum] = {tuple(mean_rewards)}\n",
    "        data['max_rewards'][testnum]  = {tuple(max_rewards)}\n",
    "        data['error'][testnum]        = {tuple(errors)}\n",
    "        data['policy'][testnum]       = {test.policy}\n",
    "        \n",
    "        print('%.2f,\\t%.2f,\\t%d,\\t%f' % (g, Time, iters, maxr))\n",
    "        \n",
    "        if showResults:\n",
    "            title = 'frozenlake_pi_' + str(rows) + 'x' + str(cols) + '_g' + str(g)\n",
    "            plot_lake(env, policy, title)\n",
    "        \n",
    "        testnum = testnum + 1\n",
    "            \n",
    "    endTime = time.time() - t0\n",
    "    print('Time taken: %.2f' % endTime)\n",
    "    \n",
    "    # see differences in policy\n",
    "    policies = data['policy']\n",
    "    \n",
    "    for i,p in enumerate(policies):\n",
    "        pol = list(p)[0]\n",
    "        steps, steps_stddev, failures = get_score(env, pol, showResults)\n",
    "        data['average_steps'][i] = steps\n",
    "        data['steps_stddev'][i]  = steps_stddev\n",
    "        data['success_pct'][i]   = 100-failures      \n",
    "        \n",
    "    # replace all nan's\n",
    "    data.fillna(0, inplace=True)\n",
    "    data.head()\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas   = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "pi_data  = policyIteration(T, R, gammas, showResults=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare number of iterations with lowest epsilon\n",
    "\n",
    "# extract the average number of steps for each gamma value\n",
    "x = gammas\n",
    "y = pi_data['average_steps']\n",
    "sigma = pi_data['steps_stddev']\n",
    "\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "plt.plot(x, y, 'o-')\n",
    "plt.fill_between(x, y-sigma, y+sigma, color='b', alpha=0.1)\n",
    "\n",
    "plt.title('Average Steps Taken vs. Gamma')\n",
    "plt.xlabel('Gamma')\n",
    "plt.ylabel('Average Steps')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare success of runs\n",
    "\n",
    "# extract the average success percentage for each gamma value\n",
    "x = pi_data['gamma']\n",
    "y = pi_data['success_pct']\n",
    "\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "plt.plot(x, y, 'o-')\n",
    "\n",
    "plt.title('Success vs. Gamma')\n",
    "plt.xlabel('Gamma')\n",
    "plt.ylabel('Success %')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot most successful gamma\n",
    "\n",
    "# find the score with the highest success percentage and get its policy\n",
    "bestRun = pi_data['success_pct'].argmax()\n",
    "\n",
    "best_policy = pi_data['policy'][bestRun]\n",
    "best_policy = np.array(list(best_policy)[0])\n",
    "best_policy = best_policy.reshape(rows, cols)\n",
    "\n",
    "# plot the policy\n",
    "title='Frozen Lake PI Optimal Policy'\n",
    "plot_lake(env, best_policy, title)\n",
    "print('Best Result:\\n\\tSuccess = %.2f\\n\\tGamma = %.2f' % (pi_data['success_pct'].max(), pi_data['gamma'][bestRun]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qLearning(t, r, gammas, alphas, alpha_decays=[0.99], epsilon_decays=[0.99], n_iterations=[10000000], showResults=False):\n",
    "    # create data structure to save off\n",
    "    columns = ['gamma', 'alpha', 'alpha_decay', 'epsilon_decay', 'iterations', 'time', 'reward', 'average_steps', 'steps_stddev', 'success_pct', 'policy', 'mean_rewards', 'max_rewards', 'error']\n",
    "    numTests = len(gammas)*len(alphas)*len(alpha_decays)*len(epsilon_decays)*len(n_iterations)\n",
    "    data = pd.DataFrame(0.0, index=np.arange(numTests), columns=columns)\n",
    "    \n",
    "    print('Gamma,\\tAlpha,\\tTime,\\tIter,\\tReward')\n",
    "    print(80*'_')\n",
    "    \n",
    "    testNum = 0\n",
    "    for g in gammas:\n",
    "        for a in alphas:\n",
    "            for a_decay in alpha_decays:\n",
    "                for e_decay in epsilon_decays:\n",
    "                    for n in n_iterations:\n",
    "                        print('Test Num %d/%d' %(testNum+1, numTests))\n",
    "                        print('Gamma: %.2f,\\tAlpha: %.2f,\\tAlpha Decay:%.3f,\\tEpsilon Decay:%.3f,\\tIterations:%d' \n",
    "                             %(g, a, a_decay, e_decay, n))\n",
    "                        \n",
    "                        test = QLearning(t, r, gamma=g, alpha=a, alpha_decay=a_decay, epsilon_decay=e_decay, n_iter=n)\n",
    "                        \n",
    "                        runs  = test.run()\n",
    "                        time  = runs[-1]['Time']\n",
    "                        iters = runs[-1]['Iteration']\n",
    "                        maxR  = runs[-1]['Max V']\n",
    "                        \n",
    "                        max_rewards, mean_rewards, errors = [], [], []\n",
    "                        for run in runs:\n",
    "                            max_rewards.append(run['Max V'])\n",
    "                            mean_rewards.append(run['Mean V'])\n",
    "                            errors.append(run['Error'])\n",
    "                        \n",
    "                        policy = np.array(test.policy)\n",
    "                        policy = policy.reshape(4,4)\n",
    "                        \n",
    "                        data['gamma'][testNum]         = g\n",
    "                        data['alpha'][testNum]         = a\n",
    "                        data['alpha_decay'][testNum]   = a_decay\n",
    "                        data['epsilon_decay'][testNum] = e_decay\n",
    "                        data['time'][testNum]          = time\n",
    "                        data['iterations'][testNum]    = iters\n",
    "                        data['reward'][testNum]        = maxR\n",
    "                        data['mean_rewards'][testNum]  = {tuple(mean_rewards)}\n",
    "                        data['max_rewards'][testNum]   = {tuple(max_rewards)}\n",
    "                        data['error'][testNum]         = {tuple(errors)}\n",
    "                        data['policy'][testNum]        = {test.policy}\n",
    "                        \n",
    "                        print('%.2f,\\t%.2f,\\t%.2f,\\t%d,\\t%f' % (g, a, time, iters, maxR))\n",
    "                        \n",
    "                        if showResults:\n",
    "                            title = 'FrozenLake_QL_' + str(rows) + 'x' + str(cols) + '_g' + str(g) + '_a' + str(a) + '_adecay' + str(a_decay) + '_edecay' + str(e_decay) + '_iter' + str(n)\n",
    "                            plot_lake(env, policy, title)\n",
    "                        \n",
    "                        testNum = testNum + 1\n",
    "            \n",
    "    # See differences in policy\n",
    "    policies = data['policy']\n",
    "    \n",
    "    for i,p in enumerate(policies):\n",
    "        pol = list(p)[0]\n",
    "        steps, steps_stddev, failures = get_score(env, pol, showResults)\n",
    "        data['average_steps'][i] = steps\n",
    "        data['steps_stddev'][i]  = steps_stddev\n",
    "        data['success_pct'][i]   = 100-failures      \n",
    "        \n",
    "    # replace all NaN's\n",
    "    data.fillna(0, inplace=True)\n",
    "    data.head()\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gammas   = [0.8, 0.9, 0.99]\n",
    "alphas   = [0.01, 0.1, 0.2]\n",
    "alpha_decays = [0.9, 0.999]\n",
    "epsilon_decays = [0.9, 0.999]\n",
    "iterations = [1e5, 1e6, 1e7]\n",
    "\n",
    "ql_data  = qLearning(T, R, gammas, alphas, alpha_decays=alpha_decays, epsilon_decays=epsilon_decays, n_iterations=iterations, showResults=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"white\")\n",
    "fig, ax = plt.subplots(figsize=(8,7))\n",
    "ax.set_title('Correlation Matrix of Q-Learning Parameters', fontsize=20)\n",
    "mask = np.triu(np.ones_like(ql_corr, dtype=np.bool))\n",
    "cmap = sns.diverging_palette(255, 0, as_cmap=True)\n",
    "sns.heatmap(ql_corr, mask=mask, cmap=cmap, square=True, linewidths=0.5, cbar_kws={\"shrink\":.75})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot values vs. Iterations\n",
    "plot_QL('iterations', 'time', title='Mean Time Taken vs. Iterations', logscale=True)\n",
    "plot_QL('iterations', 'success_pct', title='Mean Success Percentage vs. Iterations', logscale=True)\n",
    "\n",
    "# Plot alpha decay vs success pct\n",
    "plot_QL('alpha_decay', 'success_pct', title='Mean Success Percentage vs. Alpha Decay')\n",
    "\n",
    "# Plot results vs. gamma\n",
    "plot_QL('gamma', 'reward', title='Mean Success Percentage vs. Gamma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot most successful gamma\n",
    "\n",
    "# find the score with the highest success percentage and get its policy\n",
    "bestRun = ql_data['success_pct'].argmax()\n",
    "\n",
    "best_policy = ql_data['policy'][bestRun]\n",
    "\n",
    "# reshape the policy since we pulled from a csv file\n",
    "best_policy = best_policy[1:-1]\n",
    "best_policy = eval(best_policy)\n",
    "best_policy = np.array(best_policy)\n",
    "best_policy = best_policy.reshape(rows, cols)\n",
    "\n",
    "# plot the policy\n",
    "title='Frozen Lake QL Optimal Policy'\n",
    "plot_lake(env, best_policy, title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
